{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling on OpenStreetMaps data\n",
    "In this project -- which is part of the Udacity Data Analysis Nanodegree -- I will apply some data munging techniques, such as assessing the quality of the data for validity, accuracy, completeness, consistency and uniformity, to clean an specific area from OpenStreetMap data. After it, in order to try database manipulation in Python, I will load the cleaned data to a MongoDB collection (installed locally in my machine) and apply some simple statistics on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing an OpenStreetRegion: Missões!\n",
    "My region of interest in this project is Santo Ângelo, a countryside small city in the southest estate of Brazil which were my birthplace. However, since there's few data for this city and, for this project, I'm supposed to deal with databases larger than 50MB, I will consider all the neighboring cities, which in turn constitute the \"Missões\" region [1] and represent an important chapter in the South American history, since the first settlements were founded during the Spanish colonial missions [2].  \n",
    "\n",
    "Although today there are 46 municipalities composing this region, in the early eighteenth century there were only 7 villages, nowadays known in Portuguese as the \"Sete Povos das Missões\":\n",
    "- São Miguel das Missões  \n",
    "- Santo Ângelo  \n",
    "- São Borja  \n",
    "- São Nicolau  \n",
    "- São Luiz Gonzaga  \n",
    "- São Lourenço  \n",
    "- Entre-Ijuís (where remains the ruins of the town of São João Batista)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "# Dataset file name:\n",
    "FILENAME = 'Missoes.osm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#% Regular expression functions:\n",
    "def avalia_regex(dataset, regex, nsamples=True, returnList=False):\n",
    "    '''Função auxiliar para avaliar o resultado de uma dada expressão regular (regex) em um texto (string type). \n",
    "    Syntaxe: avalia_regex(dataset, regex, nsamples=True, returnList=False),\n",
    "        dataset = texto tipo string que será avaliado;\n",
    "        regex = expressão regular a ser encontrada;\n",
    "        nsamples = True, mostra todas as amostras encontradas e, em caso contrário, apenas as 3 primeiras, se houver. \n",
    "        returnList = false. Se veradeiro, retorna uma lista com as expressões encontradas.\n",
    "        \n",
    "    Exemplo de uso: avalia_regex(t03, '\\.{2,}\\s')'''\n",
    "    filtro = re.findall(regex, dataset)\n",
    "    count = len(filtro)\n",
    "    print('Foram encontrados {} matches para a expressão \"{}.\"'.format(count, regex))\n",
    "    if nsamples:\n",
    "        for item in filtro:\n",
    "            print(item, end=', ')\n",
    "    else:\n",
    "        if count > 2:\n",
    "            print('\\te.g.: {}, {}, {}.'.format(filtro[0], filtro[1], filtro[2]))\n",
    "        elif count > 0:\n",
    "            print('\\te.g.: {}.'.format(filtro[0]))\n",
    "    if returnList:\n",
    "        return filtro\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def substitui_regex(dataset, regex, subst):\n",
    "    '''Função auxiliar para substituir a coincidência de uma dada expressão regular (regex) em um texto (string type). \n",
    "    Syntaxe: avalia_regex(dataset, regex, subst), onde dataset é o texto tipo string; regex é a expressão regular a ser encontrada; subst é a string a qual será substituída. A função retorna a nova string.'''\n",
    "    filtro = re.findall(regex, dataset)\n",
    "    count = len(filtro)\n",
    "    print('Foram encontrados {} matches para a expressão \"{}.\"'.format(count, regex))\n",
    "    newText = re.sub(regex, subst, dataset)\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and reading the data:\n",
    "The data was obtained from ... \n",
    "\n",
    "After downloading the data, the first step I should do if I did not know the data model would be a simple \"less\" shell command to figure out what kind of data were in it. Since OpenStreetMaps provides us with a data model, which in turn tells us how the information is organized inside the database, we get to know that the information we are interested in are in keys called 'tag'. Just to check how many of them we will have to process on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 15589,\n",
      " 'meta': 1,\n",
      " 'nd': 444388,\n",
      " 'node': 380322,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 949,\n",
      " 'tag': 93511,\n",
      " 'way': 45320}\n"
     ]
    }
   ],
   "source": [
    "#%% Getting acquainted to the dataset\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for event, element in ET.iterparse(filename):\n",
    "        if element.tag not in tags:\n",
    "            tags[element.tag] = 1\n",
    "        else: \n",
    "            tags[element.tag] += 1\n",
    "    return tags\n",
    "\n",
    "tags = count_tags(FILENAME)\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 93.511 tags we'll be dealing with in the next steps. We can move forward to the next step: starting to audit our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing data:  \n",
    "The auditing questions comes when we start exploring the data or, if it's the case we have a prior knowledge of the problem, we already have in mind some issues to investigate. Considering there are available on Internet some similar analysis on OpenStreetMap data [3,4]; and also considering my previous knowledge about this region, I intend to audit the following issues:  \n",
    "- Are the cities names correct?\n",
    "- Are the street names correct?\n",
    "- Are there abbreviations?\n",
    "- Are the postal codes consistent?  \n",
    "\n",
    "It must be said that here the data are being first explored iteratively. Besides it is recommended to have one script for each field that is being audited, the whole process will be done through this Jupyter notebook in order to give an overview of the cleaning process. At the end, the code will be transferred to a standalone Python script (.py), in order to facilitate its automation when converting, cleaning and exporting data to a MongoDB collection, for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the cities names correct?\n",
    "In order to answer this question, I need first to know where this information is in the OpenStreetMaps (OSM) data model, which can be found in [5]. Consulting the documentation we get to know we are looking for the *addr:city* key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 distinct cities in the dataset.\n",
      "['santa rosa', 'condor', 'ijuí', 'panambi', 'santo ângelo', 'três de maio', 'panambi - rs', 'santo cristo', 'eugênio de castro', 'santo augusto', 'santo angelo', 'cruz alta', 'vila sírio', 'cerro largo', 'são josé do mauá', 'são miguel das missões', 'horizontina', 'ijui']\n"
     ]
    }
   ],
   "source": [
    "#%% Finding the cities in the dataset\n",
    "def list_cities(filename):\n",
    "    cities = []\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        if elem.tag == 'tag':\n",
    "            k = elem.attrib['k']\n",
    "            v = elem.attrib['v'].lower()  #Lowering the uppercase text\n",
    "            if k == 'addr:city':\n",
    "                if v not in cities:\n",
    "                    cities.append(v)\n",
    "    print('There are {0} distinct cities in the dataset.'.format(len(cities)))\n",
    "    return cities\n",
    "\n",
    "cities = list_cities(FILENAME)\n",
    "print(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Same cities are recorded with distinct names due to hyphenization or accentuation\n",
    "Even though I choose to use lowercase text, there are cities whose names are written with accentuation or hyphenized with the State abbreviation. A possible way to fix it is mapping the correct name to each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Cleaning the cities names:\n",
    "expected_cities = ['santa rosa', 'condor', 'ijuí', 'panambi', 'santo ângelo', 'três de maio',\n",
    "            'santo cristo', 'eugênio de castro', 'santo augusto', 'cruz alta', 'vila sírio', \n",
    "            'cerro largo', 'são josé do mauá', 'são miguel das missões', 'horizontina']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_cities(expected_cities, filename):\n",
    "    weird = []\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        if elem.tag == 'tag':\n",
    "            k = elem.attrib['k']\n",
    "            v = elem.attrib['v'].lower()  #Lowering the uppercase text\n",
    "            if k == 'addr:city':\n",
    "                if v not in expected_cities:\n",
    "                    weird.append(v)\n",
    "    weird = set(weird)\n",
    "    print('There are {0} not expected cities in the dataset.'.format(len(weird)))\n",
    "    return weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 not expected cities in the dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ijui', 'panambi - rs', 'santo angelo'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit_cities(expected_cities, FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the whole block of code, I could define the mapping:\n",
    "mapping_cities = {'ijui': 'ijuí',\n",
    "                  'santo angelo': 'santo ângelo',\n",
    "                  'panambi - rs': 'panambi'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When exporting data, the cities names must be corrected.\n",
    "def update_city(name, mapping):\n",
    "    for key in mapping:\n",
    "        if key in name:\n",
    "            return name.replace(key, mapping[key])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the street names correct? Are there any abbreviation?\n",
    "We will now iterate over all the registers to find wrong street names or abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = [\"Rua\", \"Avenida\", \"Praça\", \"Via\", \"Estrada\", \"Travessa\", \"Linha\", \"Alameda\", \"Largo\", \"Parque\", \"Rodovia\"]\n",
    "\n",
    "### IMPORTANT: Brazilian street types are in the beginning of the phrase:\n",
    "street_type_re = re.compile(r'^\\b\\S+\\.?', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    st_types = audit(FILENAME)\n",
    "    pprint.pprint(dict(st_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'14': {'14 de Julho'},\n",
      " '15': {'15 de Novembro'},\n",
      " 'Av.': {'Av. Santa Bárbara', 'Av. Gustav Kuhlmann'},\n",
      " 'BR': {'BR 285'},\n",
      " 'BR-285': {'BR-285'},\n",
      " 'BR-392': {'BR-392'},\n",
      " 'BR158': {'BR158'},\n",
      " 'Dom': {'Dom Pedro II'},\n",
      " 'ERS-342': {'ERS-342'},\n",
      " 'Getúlio': {'Getúlio Vargas'},\n",
      " 'Padre': {'Padre Afonso Rodrigues'},\n",
      " 'Paulo': {'Paulo Klemann'},\n",
      " 'RS': {'RS 218'},\n",
      " 'Santa': {'Santa Lucia'}}\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 'weirdos' found above, I will now write some mapping to clean the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the whole block of code, I could define the mapping:\n",
    "mapping = {'Av.': 'Avenida',\n",
    "           'BR ': 'BR-',\n",
    "           'BR158': 'BR-158',\n",
    "           'ERS-': 'RS-',\n",
    "           'RS ': 'RS-'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    for key in mapping:\n",
    "        if key in name:\n",
    "            return name.replace(key, mapping[key])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    st_types = audit(FILENAME)\n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print(name, \"=>\", better_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'14': {'14 de Julho'},\n",
      " '15': {'15 de Novembro'},\n",
      " 'Av.': {'Av. Santa Bárbara', 'Av. Gustav Kuhlmann'},\n",
      " 'BR': {'BR 285'},\n",
      " 'BR-285': {'BR-285'},\n",
      " 'BR-392': {'BR-392'},\n",
      " 'BR158': {'BR158'},\n",
      " 'Dom': {'Dom Pedro II'},\n",
      " 'ERS-342': {'ERS-342'},\n",
      " 'Getúlio': {'Getúlio Vargas'},\n",
      " 'Padre': {'Padre Afonso Rodrigues'},\n",
      " 'Paulo': {'Paulo Klemann'},\n",
      " 'RS': {'RS 218'},\n",
      " 'Santa': {'Santa Lucia'}}\n",
      "Getúlio Vargas => Getúlio Vargas\n",
      "BR-285 => BR-285\n",
      "Av. Santa Bárbara => Avenida Santa Bárbara\n",
      "Av. Gustav Kuhlmann => Avenida Gustav Kuhlmann\n",
      "BR 285 => BR-285\n",
      "RS 218 => RS-218\n",
      "15 de Novembro => 15 de Novembro\n",
      "14 de Julho => 14 de Julho\n",
      "BR-392 => BR-392\n",
      "ERS-342 => RS-342\n",
      "Padre Afonso Rodrigues => Padre Afonso Rodrigues\n",
      "Dom Pedro II => Dom Pedro II\n",
      "BR158 => BR-158\n",
      "Santa Lucia => Santa Lucia\n",
      "Paulo Klemann => Paulo Klemann\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the postal codes consistent?\n",
    "The Brazilian postal codes are known as CEP and must contain 8 digits in the following format '00000-000'. Furthermore, in the Missões region the CEP numbers always start with the digit 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cep = re.compile(r\"[0-9]{5}-[0-9]{3}\") #Alternative: cep = re.compile('d{5}-d{3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cep(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_cep(filename):\n",
    "    osm_file = open(filename, \"r\")\n",
    "    bad_cep = []\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_cep(tag):\n",
    "                    v = tag.attrib['v']\n",
    "                    if cep.match(v):\n",
    "                        pass\n",
    "                    else:\n",
    "                        bad_cep.append(v)\n",
    "    return bad_cep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['98910000']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit_cep(FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the whole block of code, I could define following the mapp:\n",
    "mapping_cep = {'98910000': '98910-000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When exporting data, the cities names must be corrected.\n",
    "def update_cep(name, mapping):\n",
    "    for key in mapping:\n",
    "        if key in name:\n",
    "            return name.replace(key, mapping[key])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning, shaping and exporting data\n",
    "After auditing and deciding which data must be cleaned, now it's time to shape the data into the model that will be exported to a MongoDB collection.  \n",
    "\n",
    "The output should be a list of dictionaries that look like this:\n",
    ">*{  \n",
    "\"id\": \"2406124091\",  \n",
    "\"type: \"node\",  \n",
    "\"visible\":\"true\",  \n",
    "\"created\": {  \n",
    "          \"version\":\"2\",  \n",
    "          \"changeset\":\"17206049\",  \n",
    "          \"timestamp\":\"2013-08-03T16:43:42Z\",  \n",
    "          \"user\":\"linuxUser16\",  \n",
    "          \"uid\":\"1219059\"  \n",
    "        },  \n",
    "\"pos\": [41.9757030, -87.6921867],  \n",
    "\"address\": {  \n",
    "          \"housenumber\": \"5157\",  \n",
    "          \"postcode\": \"60625\",  \n",
    "          \"street\": \"North Lincoln Ave\"  \n",
    "        },  \n",
    "\"amenity\": \"restaurant\",  \n",
    "\"cuisine\": \"mexican\",  \n",
    "\"name\": \"La Cabana De Don Luis\",  \n",
    "\"phone\": \"1 (773)-271-5176\"  \n",
    "}*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are instructions from the code given by Udacity team for this project:  \n",
    "\n",
    "*You have to complete the function 'shape_element'.\n",
    "We have provided a function that will parse the map file, and call the function with the element\n",
    "as an argument. You should return a dictionary, containing the shaped data for that element.\n",
    "We have also provided a way to save the data in a file, so that you could use\n",
    "mongoimport later on to import the shaped data into MongoDB.\n",
    "Note that in this exercise we do not use the 'update street name' procedures\n",
    "you worked on in the previous exercise. If you are using this code in your final\n",
    "project, you are strongly encouraged to use the code from previous exercise to\n",
    "update the street names before you save them to JSON.\n",
    "In particular the following things should be done:*  \n",
    "\n",
    "- you should process only 2 types of top level tags: \"node\" and \"way\"\n",
    "- all attributes of \"node\" and \"way\" should be turned into regular key/value pairs, except:\n",
    "    - attributes in the CREATED array should be added under a key \"created\"\n",
    "    - attributes for latitude and longitude should be added to a \"pos\" array,\n",
    "      for use in geospacial indexing. Make sure the values inside \"pos\" array are floats\n",
    "      and not strings.\n",
    "- if the second level tag \"k\" value contains problematic characters, it should be ignored\n",
    "- if the second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
    "- if the second level tag \"k\" value does not start with \"addr:\", but contains \":\", you can\n",
    "  process it in a way that you feel is best. For example, you might split it into a two-level\n",
    "  dictionary like with \"addr:\", or otherwise convert the \":\" to create a valid key.\n",
    "- if there is a second \":\" that separates the type/direction of a street,\n",
    "  the tag should be ignored, for example:\n",
    ">< tag k=\"addr:housenumber\" v=\"5158\"/>  \n",
    ">< tag k=\"addr:street\" v=\"North Lincoln Avenue\"/>  \n",
    ">< tag k=\"addr:street:name\" v=\"Lincoln\"/>  \n",
    ">< tag k=\"addr:street:prefix\" v=\"North\"/>  \n",
    ">< tag k=\"addr:street:type\" v=\"Avenue\"/>  \n",
    ">< tag k=\"amenity\" v=\"pharmacy\"/>  \n",
    "\n",
    "Should be turned into:\n",
    ">{...  \n",
    ">\"address\": {  \n",
    ">    \"housenumber\": 5158,  \n",
    ">    \"street\": \"North Lincoln Avenue\"  \n",
    ">}  \n",
    ">\"amenity\": \"pharmacy\",  \n",
    ">...  \n",
    ">}  \n",
    "\n",
    "- for \"way\" specifically:\n",
    ">  <nd ref=\"305896090\"/>\n",
    ">  <nd ref=\"1719825889\"/>\n",
    "\n",
    "should be turned into\n",
    "> \"node_refs\": [\"305896090\", \"1719825889\"]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "ATTRIB = [\"id\", \"visible\", \"amenity\", \"cuisine\", \"name\", \"phone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'Av.': 'Avenida',\n",
    "           'BR ': 'BR-',\n",
    "           'BR158': 'BR-158',\n",
    "           'ERS-': 'RS-',\n",
    "           'RS ': 'RS-'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_cities = {'ijui': 'ijuí',\n",
    "                  'santo angelo': 'santo ângelo',\n",
    "                  'panambi - rs': 'panambi'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_cep = {'98910000': '98910-000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_element(element):\n",
    "    \"\"\"\n",
    "    Parse, validate and format node and way xml elements.\n",
    "    Return list of dictionaries\n",
    "    Keyword arguments:\n",
    "    element -- element object from xml element tree iterparse\n",
    "    \"\"\"\n",
    "    if element.tag == 'node' or element.tag == 'way':\n",
    "\n",
    "        # Add empty created dictionary and k/v = type: node/way\n",
    "        node = {'created': {}, 'type': element.tag}\n",
    "\n",
    "        # Update pos array with lat and lon\n",
    "        if 'lat' in element.attrib and 'lon' in element.attrib:\n",
    "            node['pos'] = [float(element.attrib['lat']), float(element.attrib['lon'])]\n",
    "\n",
    "        # Deal with node and way attributes\n",
    "        for k in element.attrib:\n",
    "\n",
    "            if k == 'lat' or k == 'lon':\n",
    "                continue\n",
    "            if k in CREATED:\n",
    "                node['created'][k] = element.attrib[k]\n",
    "            else:\n",
    "                # Add direct key/value items of node/way\n",
    "                node[k] = element.attrib[k]\n",
    "\n",
    "        # Deal with second level tag items\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.attrib['k']\n",
    "            v = tag.attrib['v']\n",
    "\n",
    "            # Search for problem characters in 'k' and ignore them\n",
    "            if problemchars.search(k):\n",
    "                # Add to array to print out later\n",
    "                continue\n",
    "            elif k.startswith('addr:'):\n",
    "                address = k.split(':')\n",
    "                if len(address) == 2:\n",
    "                    if 'address' not in node:\n",
    "                        node['address'] = {}\n",
    "                    node['address'][address[1]] = v\n",
    "            else:\n",
    "                node[k] = v\n",
    "\n",
    "        # Add key/value node ref from way\n",
    "        node_refs = []\n",
    "        for nd in element.iter('nd'):\n",
    "            node_refs.append(nd.attrib['ref'])\n",
    "\n",
    "        if len(node_refs) > 0:\n",
    "            node['node_refs'] = node_refs\n",
    "\n",
    "        return node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_map(file_in, pretty=False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2) + \"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_final():\n",
    "    # NOTE: if you are running this code on your computer, with a larger dataset,\n",
    "    # call the process_map procedure with pretty=False. The pretty=True option adds\n",
    "    # additional spaces to the output, making it significantly larger.\n",
    "    data = process_map('Missoes.osm', pretty=False)\n",
    "    pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration through MongoDB API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading MongoDB libraries\n",
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "## Loading Pandas for data analysis\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Acessando o banco de dados\n",
    "client = MongoClient('mongodb://localhost:27017/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbnsilveira/anaconda3/envs/analytics3/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: database_names is deprecated. Use list_database_names instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['admin', 'config', 'examples', 'local', 'udacity']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lendo os bancos de dados disponíveis:\n",
    "client.database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbnsilveira/anaconda3/envs/analytics3/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: collection_names is deprecated. Use list_collection_names instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['osm']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lendo as coleções disponíveis:\n",
    "client.udacity.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating an alias:\n",
    "db = client.udacity.osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5badbef1ef1c25cdb2e4eefb'),\n",
      " 'created': {'changeset': '23072634',\n",
      "             'timestamp': '2014-06-21T23:33:37Z',\n",
      "             'uid': '1714450',\n",
      "             'user': 'Papibaquígrafo',\n",
      "             'version': '2'},\n",
      " 'id': '282841128',\n",
      " 'pos': [-28.1743824, -54.8258869],\n",
      " 'type': 'node'}\n"
     ]
    }
   ],
   "source": [
    "## Checking the first instance:\n",
    "firstInstance = db.find_one()\n",
    "pprint.pprint(firstInstance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refreshing the data model: \n",
    ">*{  \n",
    "\"id\": \"2406124091\",  \n",
    "\"type: \"node\",  \n",
    "\"visible\":\"true\",  \n",
    "\"created\": {  \n",
    "          \"version\":\"2\",  \n",
    "          \"changeset\":\"17206049\",  \n",
    "          \"timestamp\":\"2013-08-03T16:43:42Z\",  \n",
    "          \"user\":\"linuxUser16\",  \n",
    "          \"uid\":\"1219059\"  \n",
    "        },  \n",
    "\"pos\": [41.9757030, -87.6921867],  \n",
    "\"address\": {  \n",
    "          \"housenumber\": \"5157\",  \n",
    "          \"postcode\": \"60625\",  \n",
    "          \"street\": \"North Lincoln Ave\"  \n",
    "        },  \n",
    "\"amenity\": \"restaurant\",  \n",
    "\"cuisine\": \"mexican\",  \n",
    "\"name\": \"La Cabana De Don Luis\",  \n",
    "\"phone\": \"1 (773)-271-5176\"  \n",
    "}*\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the answeres for some questions:\n",
    "- Do the dataset contains more than the 46 current cities of the Missões region?\n",
    "- Do I have information from the 6 cities evolved form the ancient villages?\n",
    "- Which city from Missões region has more streets?\n",
    "- What are the amenities and cuisines registered for this region?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which city from Missões region has more streets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6137cde4893c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find():\n",
    "    query = { \"address.city\":\"Santo Ângelo\", \"address.street\":\"Rua São Miguel\"}\n",
    "    projection = {\"_id\":0, \"name\":1}\n",
    "    teste = db.find(query, projection)\n",
    "    \n",
    "    for a in teste:\n",
    "        pprint.pprint(a)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tbnsilveira/anaconda3/envs/analytics3/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "707"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.find({\"amenity\" : {\"$exists\":1}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_amenities():\n",
    "    result = db.aggregate([\n",
    "        {'$group': {'_id': '$address.city',\n",
    "                    'amenities': {\n",
    "                        '$addToSet': '$amenity'}}}])\n",
    "    return list(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(find_amenities())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>amenities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>são miguel das missões</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>são josé do mauá</td>\n",
       "      <td>[community_centre, fuel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>horizontina</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cerro largo</td>\n",
       "      <td>[place_of_worship, restaurant, bank, universit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cruz alta</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ijuí</td>\n",
       "      <td>[bank, library, university, arts_centre, polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>[food_court, toilets, swimming_pool, courthous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vila sírio</td>\n",
       "      <td>[fuel, community_centre]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>santa rosa</td>\n",
       "      <td>[college, school, townhall, place_of_worship]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>panambi</td>\n",
       "      <td>[pharmacy, fuel, restaurant, college]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>santo ângelo</td>\n",
       "      <td>[college]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>santo cristo</td>\n",
       "      <td>[hospital, place_of_worship]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>condor</td>\n",
       "      <td>[fuel, hospital]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>três de maio</td>\n",
       "      <td>[clinic, place_of_worship, school]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>eugênio de castro</td>\n",
       "      <td>[community_centre, school, clinic, place_of_wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>santo augusto</td>\n",
       "      <td>[college]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       _id                                          amenities\n",
       "0   são miguel das missões                                                 []\n",
       "1         são josé do mauá                           [community_centre, fuel]\n",
       "2              horizontina                                                 []\n",
       "3              cerro largo  [place_of_worship, restaurant, bank, universit...\n",
       "4                cruz alta                                                 []\n",
       "5                     ijuí  [bank, library, university, arts_centre, polic...\n",
       "6                     None  [food_court, toilets, swimming_pool, courthous...\n",
       "7               vila sírio                           [fuel, community_centre]\n",
       "8               santa rosa      [college, school, townhall, place_of_worship]\n",
       "9                  panambi              [pharmacy, fuel, restaurant, college]\n",
       "10            santo ângelo                                          [college]\n",
       "11            santo cristo                       [hospital, place_of_worship]\n",
       "12                  condor                                   [fuel, hospital]\n",
       "13            três de maio                 [clinic, place_of_worship, school]\n",
       "14       eugênio de castro  [community_centre, school, clinic, place_of_wo...\n",
       "15           santo augusto                                          [college]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_streets():\n",
    "    result = db.aggregate([\n",
    "        {'$group': {'_city': '$address.city',\n",
    "                   'count': {'$address.street':1}}},\n",
    "        {'$sort': {'count':-1}}\n",
    "    ])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationFailure",
     "evalue": "The field '_city' must be an accumulator object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationFailure\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-49100b408b44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmost_streets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-2f3b57b0f55e>\u001b[0m in \u001b[0;36mmost_streets\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         {'$group': {'_city': '$address.city',\n\u001b[1;32m      4\u001b[0m                    'count': {'$address.street':1}}},\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;34m{\u001b[0m\u001b[0;34m'$sort'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     ])\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tbnsilveira/anaconda3/envs/analytics3/lib/python3.6/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, pipeline, session, **kwargs)\u001b[0m\n\u001b[1;32m   2395\u001b[0m                                    \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m                                    \u001b[0mexplicit_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m                                    **kwargs)\n\u001b[0m\u001b[1;32m   2398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maggregate_raw_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tbnsilveira/anaconda3/envs/analytics3/lib/python3.6/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36m_aggregate\u001b[0;34m(self, pipeline, cursor_class, first_batch_size, session, explicit_session, **kwargs)\u001b[0m\n\u001b[1;32m   2302\u001b[0m                 \u001b[0mcollation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m                 \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2304\u001b[0;31m                 client=self.__database.client)\n\u001b[0m\u001b[1;32m   2305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"cursor\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tbnsilveira/anaconda3/envs/analytics3/lib/python3.6/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(self, dbname, spec, slave_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events)\u001b[0m\n\u001b[1;32m    577\u001b[0m                            \u001b[0mcompression_ctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                            \u001b[0muse_op_msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_msg_enabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m                            unacknowledged=unacknowledged)\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOperationFailure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tbnsilveira/anaconda3/envs/analytics3/lib/python3.6/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(sock, dbname, spec, slave_ok, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, check_keys, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 helpers._check_command_response(\n\u001b[1;32m    149\u001b[0m                     \u001b[0mresponse_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowable_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                     parse_write_concern_error=parse_write_concern_error)\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpublish\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tbnsilveira/anaconda3/envs/analytics3/lib/python3.6/site-packages/pymongo/helpers.py\u001b[0m in \u001b[0;36m_check_command_response\u001b[0;34m(response, msg, allowable_errors, parse_write_concern_error)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"%s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOperationFailure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merrmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationFailure\u001b[0m: The field '_city' must be an accumulator object"
     ]
    }
   ],
   "source": [
    "result = most_streets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of aggregation framework:\n",
    "No termo \"\\$user.screen_name\", o \\$ está aí para dizer que não é uma string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_tweets():\n",
    "    result = db.aggregate([\n",
    "        {\"$group\": {\"_id\": \"$user.screen_name\",\n",
    "                   \"count\": {\"$sum\":1} } },\n",
    "        {\"$sort\": {\"count\": -1 } } ])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymongo.command_cursor.CommandCursor object at 0x7f23c02a6be0>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    result = most_tweets()\n",
    "    pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymongo.command_cursor.CommandCursor object at 0x7f23c02a6be0>\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEBUG:\n",
    "Dado que não funcionou como eu esperava, procurei no StackOverflow pelo problema. A solução foi conforme abaixo:\n",
    "https://stackoverflow.com/questions/30333020/mongodb-pymongo-aggregate-gives-strange-output-something-about-cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pymongo.command_cursor.CommandCursor"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Primeiro verifiquei o tipo da variável:\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, depois de \"listar\" o cursor uma vez, é preciso fazer isto novamente. Vou armazenar, portanto, em uma variável:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48113"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = list(result)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': 'JBTeenageDream', 'count': 7},\n",
      " {'_id': 'mysterytrick', 'count': 7},\n",
      " {'_id': 'AliciaGrodecki', 'count': 6},\n",
      " {'_id': 'juhbs', 'count': 6},\n",
      " {'_id': 'fatcouncillor', 'count': 6},\n",
      " {'_id': 'officialjamesj', 'count': 6},\n",
      " {'_id': 'GeTeM_BrI', 'count': 6},\n",
      " {'_id': 'JPguedas', 'count': 6},\n",
      " {'_id': 'aki3072', 'count': 6}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(results[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L3-8. Who has the highest followers to friends ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_ratio():\n",
    "    result = db.aggregate([\n",
    "        {\"$match\": {\"user.friends_count\": {\"$gt\":0},\n",
    "                    \"user.followers_count\": {\"$gt\":0} } },\n",
    "        { \"$project\" : {\"ratio\": {\"$divide\": [\"$user.followers_count\",\n",
    "                                             \"$user.friends_count\"]},\n",
    "                        \"screen_name\": \"$user.screen_name\"}},\n",
    "        {\"$sort\": {\"ratio\": -1}},\n",
    "        {\"$limit\": 1} ])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "exMatch = list(highest_ratio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('5b9832ead4306a30822e1142'),\n",
       "  'ratio': 19221.5,\n",
       "  'screen_name': 'Twitterrific'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exMatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há diferentes tipos de operadores para *aggregation framework* que podem ser utilizados, conforme disponibilizados na documentação abaixo:  \n",
    "http://docs.mongodb.org/manual/reference/operator/aggregation/project/#pipe._S_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L3-9. Who included the most user mentions?\n",
    "Para resolver esta questão, vamos utilizar o operador $unwind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(db.find({\"entities.user_mentions\": {\"$size\":3}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5b9832e9d4306a30822d8d29'),\n",
      " 'contributors': None,\n",
      " 'coordinates': None,\n",
      " 'created_at': 'Thu Sep 02 18:11:30 +0000 2010',\n",
      " 'entities': {'hashtags': [],\n",
      "              'urls': [],\n",
      "              'user_mentions': [{'id': 115877519,\n",
      "                                 'indices': [48, 57],\n",
      "                                 'name': 'Lathifah Maulida',\n",
      "                                 'screen_name': 'fremolio'},\n",
      "                                {'id': 86039346,\n",
      "                                 'indices': [65, 73],\n",
      "                                 'name': \"Nya' Zata Amani\",\n",
      "                                 'screen_name': 'nyazaaa'},\n",
      "                                {'id': 59490923,\n",
      "                                 'indices': [101, 113],\n",
      "                                 'name': 'Aidina Ashura',\n",
      "                                 'screen_name': 'aidinashura'}]},\n",
      " 'favorited': False,\n",
      " 'geo': None,\n",
      " 'id': 22819404500,\n",
      " 'in_reply_to_screen_name': None,\n",
      " 'in_reply_to_status_id': None,\n",
      " 'in_reply_to_user_id': None,\n",
      " 'place': None,\n",
      " 'retweet_count': None,\n",
      " 'retweeted': False,\n",
      " 'source': '<a href=\"http://www.snaptu.com\" rel=\"nofollow\">Snaptu.com</a>',\n",
      " 'text': 'Yang ini pun ga memberikan penjelasan --&gt; RT @fremolio: --\"RT '\n",
      "         '@nyazaaa: Aku ikutan bingung -,- RT @aidinashura: Alasanya? -.-\"',\n",
      " 'truncated': False,\n",
      " 'user': {'contributors_enabled': False,\n",
      "          'created_at': 'Thu Jul 23 15:11:48 +0000 2009',\n",
      "          'description': 'I am a person, a kid, a daughter, a grandchild, a '\n",
      "                         'little sister, a cousin, a student, a friend, a '\n",
      "                         'leader, a follower, and a dreamer.',\n",
      "          'favourites_count': 964,\n",
      "          'follow_request_sent': None,\n",
      "          'followers_count': 302,\n",
      "          'following': None,\n",
      "          'friends_count': 185,\n",
      "          'geo_enabled': False,\n",
      "          'id': 59490923,\n",
      "          'lang': 'en',\n",
      "          'listed_count': 6,\n",
      "          'location': 'Indonesia',\n",
      "          'name': 'Aidina Ashura',\n",
      "          'notifications': None,\n",
      "          'profile_background_color': '505b61',\n",
      "          'profile_background_image_url': 'http://s.twimg.com/a/1282262833/images/themes/theme14/bg.gif',\n",
      "          'profile_background_tile': True,\n",
      "          'profile_image_url': 'http://a0.twimg.com/profile_images/695035924/twitterProfilePhoto_normal.jpg',\n",
      "          'profile_link_color': '058bf2',\n",
      "          'profile_sidebar_border_color': 'eeeeee',\n",
      "          'profile_sidebar_fill_color': 'efefef',\n",
      "          'profile_text_color': '000000',\n",
      "          'profile_use_background_image': True,\n",
      "          'protected': False,\n",
      "          'screen_name': 'aidinashura',\n",
      "          'show_all_inline_media': False,\n",
      "          'statuses_count': 21007,\n",
      "          'time_zone': 'Jakarta',\n",
      "          'url': 'http://aidinashura.tumblr.com/',\n",
      "          'utc_offset': 25200,\n",
      "          'verified': False}}\n"
     ]
    }
   ],
   "source": [
    "## Vejamos que há (3) \"user mentions\" neste tweet...\n",
    "pprint.pprint(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A solução utilizando 'unwind operator'\n",
    "def user_mentions():\n",
    "    result = db.aggregate([\n",
    "        {\"$unwind\": \"$entities.user_mentions\"},\n",
    "        {\"$group\": {\"_id\": \"$user.screen_name\",\n",
    "                    \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        {\"$limit\": 5}])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': 'ThizBoySwagLoud', 'count': 21},\n",
       " {'_id': 'MULA_BSB', 'count': 21},\n",
       " {'_id': 'vanilla3450', 'count': 18},\n",
       " {'_id': 'Democracy_Work', 'count': 17},\n",
       " {'_id': 'itsajstuerd', 'count': 16}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exL3_9 = list(user_mentions())\n",
    "exL3_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L3-13. Group accumulation operators\n",
    "Exemplo utilizando o operador $avg para criar uma nova chave:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_retweet_avg():\n",
    "    result = db.aggregate([\n",
    "        {\"$unwind\": \"$entities.hashtags\"},\n",
    "        {\"$group\": {\"_id\": \"$entities.hashtags.text\",\n",
    "                    \"retweet_avg\": {\"$avg\": \"$retweet_count\"}}},\n",
    "        {\"$sort\": {\"retweet_avg\": -1}},\n",
    "        {\"$limit\": 5}])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': 'NowBumpin', 'retweet_avg': None},\n",
       " {'_id': 'loveu', 'retweet_avg': None},\n",
       " {'_id': 'reptilenews', 'retweet_avg': None},\n",
       " {'_id': 'ChatterFact', 'retweet_avg': None},\n",
       " {'_id': 'gettinstrong', 'retweet_avg': None}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exL3_13 = list(hashtag_retweet_avg())\n",
    "exL3_13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo abaixo, o mesmo problema é colocado porém agora por usuários, utilizando o operador $addToSet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_hashtags_by_user():\n",
    "    result = db.aggregate([\n",
    "        {\"$unwind\": \"$entities.hashtags\"},\n",
    "        {\"$group\": {\"_id\": \"$user.screen_name\",\n",
    "                    \"unique_hashtags\": {\n",
    "                        \"$addToSet\": \"$entities.hashtags.text\"}}},\n",
    "        {\"$sort\": {\"_id\": -1}},\n",
    "        {\"$limit\": 5}])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': 'zzj090728', 'unique_hashtags': ['yaplog']},\n",
       " {'_id': 'zyosouzai', 'unique_hashtags': ['lv25834899']},\n",
       " {'_id': 'zudyezezwe65',\n",
       "  'unique_hashtags': ['line', 'all', 'data', 'mun', 'pop', 'kl']},\n",
       " {'_id': 'zorapataki', 'unique_hashtags': ['zorapataki']},\n",
       " {'_id': 'zootcadillac', 'unique_hashtags': ['TwitterJokeTrial']}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exL3_13b = list(unique_hashtags_by_user())\n",
    "exL3_13b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L3-15. Who has mentioned the most unique users?\n",
    "(...) we're only counting mentions of users not already accounted for in our grouping. So here's the code for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_mentions_v2():\n",
    "    result = db.aggregate([\n",
    "        {\"$unwind\": \"$entities.user_mentions\"},\n",
    "        {\"$group\": {\n",
    "            \"_id\": \"$user.screen_name\",\n",
    "            \"mset\": {\n",
    "                \"$addToSet\": \"$entities.user_mentions.screen_name\"\n",
    "            }}},\n",
    "        {\"$unwind\": \"$mset\"},\n",
    "        {\"$group\": {\"_id\": \"$_id\", \n",
    "                    \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        {\"$limit\": 10}])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': 'Democracy_Work', 'count': 17},\n",
       " {'_id': 'ThizBoySwagLoud', 'count': 16},\n",
       " {'_id': 'FollowersNeeded', 'count': 15},\n",
       " {'_id': 'itsajstuerd', 'count': 15},\n",
       " {'_id': 'Egreeedy', 'count': 12},\n",
       " {'_id': 'Spirics', 'count': 12},\n",
       " {'_id': 'burnedoutdoc', 'count': 11},\n",
       " {'_id': 'MULA_BSB', 'count': 11},\n",
       " {'_id': 'soulbrother64', 'count': 11},\n",
       " {'_id': 'uckervaiprostt', 'count': 10}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exL3_15 = list(user_mentions_v2())\n",
    "exL3_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] https://en.wikipedia.org/wiki/Miss%C3%B5es  \n",
    "[2] https://en.wikipedia.org/wiki/Spanish_missions_in_South_America  \n",
    "[3] https://jasonicarter.github.io/openstreetmap-data-wrangling-mongodb/  \n",
    "[4] https://eberlitz.github.io/2015/09/18/data-wrangle-openstreetmaps-data/  \n",
    "[5] https://wiki.openstreetmap.org/wiki/Key:addr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACKUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Application issues:   \n",
    "- Do the dataset contains more than the 46 current cities of the Missões region?\n",
    "- Do I have information from the 6 cities evolved form the ancient villages?\n",
    "\n",
    "B. Data issues:  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analytics3]",
   "language": "python",
   "name": "conda-env-analytics3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

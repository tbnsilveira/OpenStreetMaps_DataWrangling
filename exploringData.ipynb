{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling on OpenStreetMaps data\n",
    "In this project -- which is part of the Udacity Data Analysis Nanodegree -- I will apply some data munging techniques, such as assessing the quality of the data for validity, accuracy, completeness, consistency and uniformity, to clean an specific area from OpenStreetMap data. After it, in order to try database manipulation in Python, I will load the cleaned data to a MongoDB collection (installed locally in my machine) and apply some simple statistics on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing an OpenStreetRegion: Missões!\n",
    "My region of interest in this project is Santo Ângelo, a countryside small city in the southest estate of Brazil which were my birthplace. However, since there's few data for this city and, for this project, I'm supposed to deal with databases larger than 50MB, I will consider all the neighboring cities, which in turn constitute the \"Missões\" region [1] and represent an important chapter in the South American history, since the first settlements were founded during the Spanish colonial missions [2].  \n",
    "\n",
    "Although today there are 46 municipalities composing this region, in the early eighteenth century there were only 7 villages, nowadays known in Portuguese as the \"Sete Povos das Missões\":\n",
    "- São Miguel das Missões  \n",
    "- Santo Ângelo  \n",
    "- São Borja  \n",
    "- São Nicolau  \n",
    "- São Luiz Gonzaga  \n",
    "- São Lourenço  \n",
    "- Entre-Ijuís (where remains the ruins of the town of São João Batista)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some basic statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "# Dataset file name:\n",
    "FILENAME = 'Missoes.osm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#% Regular expression functions:\n",
    "def avalia_regex(dataset, regex, nsamples=True, returnList=False):\n",
    "    '''Função auxiliar para avaliar o resultado de uma dada expressão regular (regex) em um texto (string type). \n",
    "    Syntaxe: avalia_regex(dataset, regex, nsamples=True, returnList=False),\n",
    "        dataset = texto tipo string que será avaliado;\n",
    "        regex = expressão regular a ser encontrada;\n",
    "        nsamples = True, mostra todas as amostras encontradas e, em caso contrário, apenas as 3 primeiras, se houver. \n",
    "        returnList = false. Se veradeiro, retorna uma lista com as expressões encontradas.\n",
    "        \n",
    "    Exemplo de uso: avalia_regex(t03, '\\.{2,}\\s')'''\n",
    "    filtro = re.findall(regex, dataset)\n",
    "    count = len(filtro)\n",
    "    print('Foram encontrados {} matches para a expressão \"{}.\"'.format(count, regex))\n",
    "    if nsamples:\n",
    "        for item in filtro:\n",
    "            print(item, end=', ')\n",
    "    else:\n",
    "        if count > 2:\n",
    "            print('\\te.g.: {}, {}, {}.'.format(filtro[0], filtro[1], filtro[2]))\n",
    "        elif count > 0:\n",
    "            print('\\te.g.: {}.'.format(filtro[0]))\n",
    "    if returnList:\n",
    "        return filtro\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def substitui_regex(dataset, regex, subst):\n",
    "    '''Função auxiliar para substituir a coincidência de uma dada expressão regular (regex) em um texto (string type). \n",
    "    Syntaxe: avalia_regex(dataset, regex, subst), onde dataset é o texto tipo string; regex é a expressão regular a ser encontrada; subst é a string a qual será substituída. A função retorna a nova string.'''\n",
    "    filtro = re.findall(regex, dataset)\n",
    "    count = len(filtro)\n",
    "    print('Foram encontrados {} matches para a expressão \"{}.\"'.format(count, regex))\n",
    "    newText = re.sub(regex, subst, dataset)\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and reading the data:\n",
    "The data was obtained from ... \n",
    "\n",
    "After downloading the data, the first step I should do if I did not know the data model would be a simple \"less\" shell command to figure out what kind of data were in it. Since OpenStreetMaps provides us with a data model, which in turn tells us how the information is organized inside the database, we get to know that the information we are interested in are in keys called 'tag'. Just to check how many of them we will have to process on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 15589,\n",
      " 'meta': 1,\n",
      " 'nd': 444388,\n",
      " 'node': 380322,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 949,\n",
      " 'tag': 93511,\n",
      " 'way': 45320}\n"
     ]
    }
   ],
   "source": [
    "#%% Getting acquainted to the dataset\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for event, element in ET.iterparse(filename):\n",
    "        if element.tag not in tags:\n",
    "            tags[element.tag] = 1\n",
    "        else: \n",
    "            tags[element.tag] += 1\n",
    "    return tags\n",
    "\n",
    "tags = count_tags(FILENAME)\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 93.511 tags we'll be dealing with in the next steps. We can move forward to the next step: starting to audit our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing data:  \n",
    "The auditing questions comes when we start exploring the data or, if it's the case we have a prior knowledge of the problem, we already have in mind some issues to investigate. Considering there are available on Internet some similar analysis on OpenStreetMap data [3,4]; and also considering my previous knowledge about this region, I intend to audit the following issues:  \n",
    "- Are the cities names correct?\n",
    "- Are the street names correct?\n",
    "- Are there abbreviations?\n",
    "- Are the postal codes consistent?  \n",
    "\n",
    "It must be said that here the data are being first explored iteratively. Besides it is recommended to have one script for each field that is being audited, the whole process will be done through this Jupyter notebook in order to give an overview of the cleaning process. At the end, the code will be transferred to a standalone Python script (.py), in order to facilitate its automation when converting, cleaning and exporting data to a MongoDB collection, for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the cities names correct?\n",
    "In order to answer this question, I need first to know where this information is in the OpenStreetMaps (OSM) data model, which can be found in [5]. Consulting the documentation we get to know we are looking for the *addr:city* key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 distinct cities in the dataset.\n",
      "['santa rosa', 'condor', 'ijuí', 'panambi', 'santo ângelo', 'três de maio', 'panambi - rs', 'santo cristo', 'eugênio de castro', 'santo augusto', 'santo angelo', 'cruz alta', 'vila sírio', 'cerro largo', 'são josé do mauá', 'são miguel das missões', 'horizontina', 'ijui']\n"
     ]
    }
   ],
   "source": [
    "#%% Finding the cities in the dataset\n",
    "def list_cities(filename):\n",
    "    cities = []\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        if elem.tag == 'tag':\n",
    "            k = elem.attrib['k']\n",
    "            v = elem.attrib['v'].lower()  #Lowering the uppercase text\n",
    "            if k == 'addr:city':\n",
    "                if v not in cities:\n",
    "                    cities.append(v)\n",
    "    print('There are {0} distinct cities in the dataset.'.format(len(cities)))\n",
    "    return cities\n",
    "\n",
    "cities = list_cities(FILENAME)\n",
    "print(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Same cities are recorded with distinct names due to hyphenization or accentuation\n",
    "Even though I choose to use lowercase text, there are cities whose names are written with accentuation or hyphenized with the State abbreviation. A possible way to fix it is mapping the correct name to each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Cleaning the cities names:\n",
    "expected_cities = ['santa rosa', 'condor', 'ijuí', 'panambi', 'santo ângelo', 'três de maio',\n",
    "            'santo cristo', 'eugênio de castro', 'santo augusto', 'cruz alta', 'vila sírio', \n",
    "            'cerro largo', 'são josé do mauá', 'são miguel das missões', 'horizontina']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_cities(expected_cities, filename):\n",
    "    weird = []\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        if elem.tag == 'tag':\n",
    "            k = elem.attrib['k']\n",
    "            v = elem.attrib['v'].lower()  #Lowering the uppercase text\n",
    "            if k == 'addr:city':\n",
    "                if v not in expected_cities:\n",
    "                    weird.append(v)\n",
    "    weird = set(weird)\n",
    "    print('There are {0} not expected cities in the dataset.'.format(len(weird)))\n",
    "    return weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 not expected cities in the dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ijui', 'panambi - rs', 'santo angelo'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit_cities(expected_cities, FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the whole block of code, I could define the mapping:\n",
    "mapping_cities = {'ijui': 'ijuí',\n",
    "                  'santo angelo': 'santo ângelo',\n",
    "                  'panambi - rs': 'panambi'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When exporting data, the cities names must be corrected.\n",
    "def update_city(name, mapping):\n",
    "    for key in mapping:\n",
    "        if key in name:\n",
    "            return name.replace(key, mapping[key])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the street names correct? Are there any abbreviation?\n",
    "We will now iterate over all the registers to find wrong street names or abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = [\"Rua\", \"Avenida\", \"Praça\", \"Via\", \"Estrada\", \"Travessa\", \"Linha\", \"Alameda\", \"Largo\", \"Parque\", \"Rodovia\"]\n",
    "\n",
    "### IMPORTANT: Brazilian street types are in the beginning of the phrase:\n",
    "street_type_re = re.compile(r'^\\b\\S+\\.?', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    st_types = audit(FILENAME)\n",
    "    pprint.pprint(dict(st_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'14': {'14 de Julho'},\n",
      " '15': {'15 de Novembro'},\n",
      " 'Av.': {'Av. Santa Bárbara', 'Av. Gustav Kuhlmann'},\n",
      " 'BR': {'BR 285'},\n",
      " 'BR-285': {'BR-285'},\n",
      " 'BR-392': {'BR-392'},\n",
      " 'BR158': {'BR158'},\n",
      " 'Dom': {'Dom Pedro II'},\n",
      " 'ERS-342': {'ERS-342'},\n",
      " 'Getúlio': {'Getúlio Vargas'},\n",
      " 'Padre': {'Padre Afonso Rodrigues'},\n",
      " 'Paulo': {'Paulo Klemann'},\n",
      " 'RS': {'RS 218'},\n",
      " 'Santa': {'Santa Lucia'}}\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 'weirdos' found above, I will now write some mapping to clean the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the whole block of code, I could define the mapping:\n",
    "mapping = {'Av.': 'Avenida',\n",
    "           'BR ': 'BR-',\n",
    "           'BR158': 'BR-158',\n",
    "           'ERS-': 'RS-',\n",
    "           'RS ': 'RS-'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    for key in mapping:\n",
    "        if key in name:\n",
    "            return name.replace(key, mapping[key])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    st_types = audit(FILENAME)\n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print(name, \"=>\", better_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'14': {'14 de Julho'},\n",
      " '15': {'15 de Novembro'},\n",
      " 'Av.': {'Av. Santa Bárbara', 'Av. Gustav Kuhlmann'},\n",
      " 'BR': {'BR 285'},\n",
      " 'BR-285': {'BR-285'},\n",
      " 'BR-392': {'BR-392'},\n",
      " 'BR158': {'BR158'},\n",
      " 'Dom': {'Dom Pedro II'},\n",
      " 'ERS-342': {'ERS-342'},\n",
      " 'Getúlio': {'Getúlio Vargas'},\n",
      " 'Padre': {'Padre Afonso Rodrigues'},\n",
      " 'Paulo': {'Paulo Klemann'},\n",
      " 'RS': {'RS 218'},\n",
      " 'Santa': {'Santa Lucia'}}\n",
      "Getúlio Vargas => Getúlio Vargas\n",
      "BR-285 => BR-285\n",
      "Av. Santa Bárbara => Avenida Santa Bárbara\n",
      "Av. Gustav Kuhlmann => Avenida Gustav Kuhlmann\n",
      "BR 285 => BR-285\n",
      "RS 218 => RS-218\n",
      "15 de Novembro => 15 de Novembro\n",
      "14 de Julho => 14 de Julho\n",
      "BR-392 => BR-392\n",
      "ERS-342 => RS-342\n",
      "Padre Afonso Rodrigues => Padre Afonso Rodrigues\n",
      "Dom Pedro II => Dom Pedro II\n",
      "BR158 => BR-158\n",
      "Santa Lucia => Santa Lucia\n",
      "Paulo Klemann => Paulo Klemann\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the postal codes consistent?\n",
    "The Brazilian postal codes are known as CEP and must contain 8 digits in the following format '00000-000'. Furthermore, in the Missões region the CEP numbers always start with the digit 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cep = re.compile(r\"[0-9]{5}-[0-9]{3}\") #Alternative: cep = re.compile('d{5}-d{3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cep(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_cep(filename):\n",
    "    osm_file = open(filename, \"r\")\n",
    "    bad_cep = []\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_cep(tag):\n",
    "                    v = tag.attrib['v']\n",
    "                    if cep.match(v):\n",
    "                        pass\n",
    "                    else:\n",
    "                        bad_cep.append(v)\n",
    "    return bad_cep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['98910000']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audit_cep(FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the whole block of code, I could define following the mapp:\n",
    "mapping_cep = {'98910000': '98910-000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When exporting data, the cities names must be corrected.\n",
    "def update_cep(name, mapping):\n",
    "    for key in mapping:\n",
    "        if key in name:\n",
    "            return name.replace(key, mapping[key])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning, shaping and exporting data\n",
    "After auditing and deciding which data must be cleaned, now it's time to shape the data into the model that will be exported to a MongoDB collection.  \n",
    "\n",
    "The output should be a list of dictionaries that look like this:\n",
    ">*{  \n",
    "\"id\": \"2406124091\",  \n",
    "\"type: \"node\",  \n",
    "\"visible\":\"true\",  \n",
    "\"created\": {  \n",
    "          \"version\":\"2\",  \n",
    "          \"changeset\":\"17206049\",  \n",
    "          \"timestamp\":\"2013-08-03T16:43:42Z\",  \n",
    "          \"user\":\"linuxUser16\",  \n",
    "          \"uid\":\"1219059\"  \n",
    "        },  \n",
    "\"pos\": [41.9757030, -87.6921867],  \n",
    "\"address\": {  \n",
    "          \"housenumber\": \"5157\",  \n",
    "          \"postcode\": \"60625\",  \n",
    "          \"street\": \"North Lincoln Ave\"  \n",
    "        },  \n",
    "\"amenity\": \"restaurant\",  \n",
    "\"cuisine\": \"mexican\",  \n",
    "\"name\": \"La Cabana De Don Luis\",  \n",
    "\"phone\": \"1 (773)-271-5176\"  \n",
    "}*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are instructions from the code given by Udacity team for this project:  \n",
    "\n",
    "*You have to complete the function 'shape_element'.\n",
    "We have provided a function that will parse the map file, and call the function with the element\n",
    "as an argument. You should return a dictionary, containing the shaped data for that element.\n",
    "We have also provided a way to save the data in a file, so that you could use\n",
    "mongoimport later on to import the shaped data into MongoDB.\n",
    "Note that in this exercise we do not use the 'update street name' procedures\n",
    "you worked on in the previous exercise. If you are using this code in your final\n",
    "project, you are strongly encouraged to use the code from previous exercise to\n",
    "update the street names before you save them to JSON.\n",
    "In particular the following things should be done:*  \n",
    "\n",
    "- you should process only 2 types of top level tags: \"node\" and \"way\"\n",
    "- all attributes of \"node\" and \"way\" should be turned into regular key/value pairs, except:\n",
    "    - attributes in the CREATED array should be added under a key \"created\"\n",
    "    - attributes for latitude and longitude should be added to a \"pos\" array,\n",
    "      for use in geospacial indexing. Make sure the values inside \"pos\" array are floats\n",
    "      and not strings.\n",
    "- if the second level tag \"k\" value contains problematic characters, it should be ignored\n",
    "- if the second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
    "- if the second level tag \"k\" value does not start with \"addr:\", but contains \":\", you can\n",
    "  process it in a way that you feel is best. For example, you might split it into a two-level\n",
    "  dictionary like with \"addr:\", or otherwise convert the \":\" to create a valid key.\n",
    "- if there is a second \":\" that separates the type/direction of a street,\n",
    "  the tag should be ignored, for example:\n",
    ">< tag k=\"addr:housenumber\" v=\"5158\"/>  \n",
    ">< tag k=\"addr:street\" v=\"North Lincoln Avenue\"/>  \n",
    ">< tag k=\"addr:street:name\" v=\"Lincoln\"/>  \n",
    ">< tag k=\"addr:street:prefix\" v=\"North\"/>  \n",
    ">< tag k=\"addr:street:type\" v=\"Avenue\"/>  \n",
    ">< tag k=\"amenity\" v=\"pharmacy\"/>  \n",
    "\n",
    "Should be turned into:\n",
    ">{...  \n",
    ">\"address\": {  \n",
    ">    \"housenumber\": 5158,  \n",
    ">    \"street\": \"North Lincoln Avenue\"  \n",
    ">}  \n",
    ">\"amenity\": \"pharmacy\",  \n",
    ">...  \n",
    ">}  \n",
    "\n",
    "- for \"way\" specifically:\n",
    ">  <nd ref=\"305896090\"/>\n",
    ">  <nd ref=\"1719825889\"/>\n",
    "\n",
    "should be turned into\n",
    "> \"node_refs\": [\"305896090\", \"1719825889\"]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "ATTRIB = [\"id\", \"visible\", \"amenity\", \"cuisine\", \"name\", \"phone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'Av.': 'Avenida',\n",
    "           'BR ': 'BR-',\n",
    "           'BR158': 'BR-158',\n",
    "           'ERS-': 'RS-',\n",
    "           'RS ': 'RS-'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_cities = {'ijui': 'ijuí',\n",
    "                  'santo angelo': 'santo ângelo',\n",
    "                  'panambi - rs': 'panambi'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_cep = {'98910000': '98910-000'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_element(element):\n",
    "    \"\"\"\n",
    "    Parse, validate and format node and way xml elements.\n",
    "    Return list of dictionaries\n",
    "    Keyword arguments:\n",
    "    element -- element object from xml element tree iterparse\n",
    "    \"\"\"\n",
    "    if element.tag == 'node' or element.tag == 'way':\n",
    "\n",
    "        # Add empty created dictionary and k/v = type: node/way\n",
    "        node = {'created': {}, 'type': element.tag}\n",
    "\n",
    "        # Update pos array with lat and lon\n",
    "        if 'lat' in element.attrib and 'lon' in element.attrib:\n",
    "            node['pos'] = [float(element.attrib['lat']), float(element.attrib['lon'])]\n",
    "\n",
    "        # Deal with node and way attributes\n",
    "        for k in element.attrib:\n",
    "\n",
    "            if k == 'lat' or k == 'lon':\n",
    "                continue\n",
    "            if k in CREATED:\n",
    "                node['created'][k] = element.attrib[k]\n",
    "            else:\n",
    "                # Add direct key/value items of node/way\n",
    "                node[k] = element.attrib[k]\n",
    "\n",
    "        # Deal with second level tag items\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.attrib['k']\n",
    "            v = tag.attrib['v']\n",
    "\n",
    "            # Search for problem characters in 'k' and ignore them\n",
    "            if problemchars.search(k):\n",
    "                # Add to array to print out later\n",
    "                continue\n",
    "            elif k.startswith('addr:'):\n",
    "                address = k.split(':')\n",
    "                if len(address) == 2:\n",
    "                    if 'address' not in node:\n",
    "                        node['address'] = {}\n",
    "                    node['address'][address[1]] = v\n",
    "            else:\n",
    "                node[k] = v\n",
    "\n",
    "        # Add key/value node ref from way\n",
    "        node_refs = []\n",
    "        for nd in element.iter('nd'):\n",
    "            node_refs.append(nd.attrib['ref'])\n",
    "\n",
    "        if len(node_refs) > 0:\n",
    "            node['node_refs'] = node_refs\n",
    "\n",
    "        return node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_map(file_in, pretty=False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2) + \"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_final():\n",
    "    # NOTE: if you are running this code on your computer, with a larger dataset,\n",
    "    # call the process_map procedure with pretty=False. The pretty=True option adds\n",
    "    # additional spaces to the output, making it significantly larger.\n",
    "    data = process_map('Missoes.osm', pretty=False)\n",
    "    pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] https://en.wikipedia.org/wiki/Miss%C3%B5es  \n",
    "[2] https://en.wikipedia.org/wiki/Spanish_missions_in_South_America  \n",
    "[3] https://jasonicarter.github.io/openstreetmap-data-wrangling-mongodb/  \n",
    "[4] https://eberlitz.github.io/2015/09/18/data-wrangle-openstreetmaps-data/  \n",
    "[5] https://wiki.openstreetmap.org/wiki/Key:addr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACKUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Application issues:   \n",
    "- Do the dataset contains more than the 46 current cities of the Missões region?\n",
    "- Do I have information from the 6 cities evolved form the ancient villages?\n",
    "\n",
    "B. Data issues:  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analytics3]",
   "language": "python",
   "name": "conda-env-analytics3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
